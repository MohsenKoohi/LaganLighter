![LaganLighter](https://hpgp.net/LaganLighter/headers/header-squirrel.jpg) 

#  LaganLighter Docs: General Info

## Introducion

### Project Statement
We study the characteristics of graph datasets and their implications on the memory utilization and memory locality 
of graph analytics. We identify the connection between different vertex classes in real-world graph datasets and we 
explore how these connections affect the performance of different graph analytics and traversals. These patterns 
are used to propose novel structure-aware graph analytic algorithms with optimized performance.

### Project Steps
(1) Analysing the functionality of locality-optimizing graph reordering algorithms for different real-world graph datasets by introducing new metrics and tools (published in [IISWC'21](https://doi.org/10.1109/ISPASS51385.2021.00023) and 
[ISPASS’21](https://doi.org/10.1109/ISPASS51385.2021.00023)).

(2) Introducing locality-optimizing graph algorithms: [Hub Temporal Locality (HTL)](7.0-ihtl.md) 
algorithm as a structure-aware and cache-friendly graph traversal (published in [ICPP’21](https://doi.org/10.1145/3472456.3472462)) 
and [LOTUS](8.0-lotus.md) algorithm that optimizes locality in Triangle Counting (published in [PPoPP’22](https://doi.org/10.1145/3503221.3508402)).

(3) Introducing memory and work-optimizing algorithms: 
[Thrifty Label Propagation](2.0-thrifty.md) that 
optimizes the Connected Components algorithm for power-law graph datasets (published in [IEEE CLUSTER’21](https://doi.org/10.1109/Cluster48925.2021.00042)), [MASTIFF](3.0-mastiff.md) algorithm that optimizes work-efficiency in finding Minimum Spanning Tree/Forest (published in [ICS’22](https://doi.org/10.1145/3524059.3532365)), and [SAPCo Sort](1.0-sapco.md) that optimizes degree-ordering of power-law graphs (published in [ISPASS’22](https://doi.org/10.1109/ISPASS55109.2022.00015)).

(4) To demonstrate the dependency of performance on graph locality and computer architecture, we design performance model of graph algorithms. We apply this method to [PoTra](6.0-potra.md), our structure-aware graph transposition algorithm, illustrating how architectural characteristics can be leveraged to optimize performance.

### Project Legacy

The LaganLighter project shows that the different constituents of a network (graph) may expose different behaviours to 
analytic algorithms as a result of contrasting features and requirements. By exploiting these diverse behaviours and 
structural differences, we can accelerate the performance.

The [Uniform Memory Demands Strategy](https://pure.qub.ac.uk/files/417551607/LL_Thesis.pdf) concentrates on separating these constituents with contrasting needs and behaviours based on the degree of vertices. Similarly, other features of the skewed degree networks, or in other words, other perspectives on separating constituents of the network (such as connectivity that is used by Thrifty and MASTIFF algorithms) can be exploited to accelerate graph algorithms.


## Cloning

`git clone https://github.com/MohsenKoohi/LaganLighter.git --recursive`

## Graph Types
LaganLighter supports the following graph formats:
- CSR/CSC [WebGraph](https://law.di.unimi.it/datasets.php) format: supported by the [Poplar Graph Loading Library](https://blogs.qub.ac.uk/DIPSA/Poplar/)
- Textual CSR/CSC for small graphs (testing). This format has 4 lines: 
  1. Number of vertices (|V|), 
  2. Number of edges (|E|), 
  3. |V| space-separated numbers showing offsets of the vertices, 
  4. |E| space-separated numbers indicating edges.
   

## Measurements
In addition to execution time, we use the [PAPI](http://icl.cs.utk.edu/papi/) library to measure hardware counters such as L3 cache misses, hardware instructions, DTLB misses, and load and store memory instructions. ( `papi_(init/start/reset/stop)` and `(print/reset)_hw_events` functions defined in [omp.c](https://github.com/MohsenKoohi/LaganLighter/blob/main/omp.c) ).

To measure load balance, we measure the total time of executing a loop and the time each thread spends in this loop (`mt` and `ttimes` in the following sample code). Using these values, PTIP macro (defined in [omp.c](https://github.com/MohsenKoohi/LaganLighter/blob/main/omp.c) ) calculates the percentage of average idle time (as an indicator of load imbalance) and prints it with the total time (`mt`).
```
    mt = - get_nano_time()
    #pragma omp parallel  
    {
       unsigned tid = omp_get_thread_num();
       ttimes[tid] = - get_nano_time();
    	
       #pragma omp for nowait
       for(unsigned int v = 0; v < g->vertices_count; v++)
       {
          // .....
       }
       ttimes[tid] += get_nano_time();
    }
    mt += get_nano_time();
    PTIP("Step ... ");
 ```   

As an example, the following execution of [Thrifty](https://blogs.qub.ac.uk/DIPSA/Thrifty), shows that the “Zero Planting” step has been performed in 8.98 milliseconds and with a 8.22% load imbalance, while processors have been idle for 72.22% of the execution time, on average, in the “Initial Push” step.

![](images/load-imbalance.jpg)

## NUMA-Aware and Locality-Preserving Partitioning and Scheduling

In order to assign consecutive partitions (vertices and/or their edges) to each parallel processor, we initially divide partitions and assign a number of consecutive partitions to each thread. Then, we specify the order of victim threads in the work-stealing process. During the initialization of LaganLighter parallel processing environment (in `**initialize_omp_par_env**()` function defined in file omp.c), for each thread, we create a list of threads as consequent victims of stealing.

A thread, first, steals jobs (i.e., partitions) from consequent threads in the same NUMA node and then from the threads in consequent NUMA nodes. As an example, the following image shows the stealing order of a 24-core machine with 2 NUMA nodes. This shows that thread 1 steals from threads 2, 3, …,11, and ,0 running on the same NUMA socket and then from threads 13, 14, …, 23, and 12 running on the next NUMA socket.

![](images/stealing-order.png)

We use `**dynamic_partitioning_...()**` functions (in file partitioning.c) to process partitions by threads in the specified order. A sample code is in the following:

```
    struct dynamic_partitioning* dp = dynamic_partitioning_initialize(pe, partitions_count);
    
    #pragma omp parallel  
    {
       unsigned int tid = omp_get_thread_num();
       unsigned int partition = -1U;		
    
       while(1)
       {
          partition = dynamic_partitioning_get_next_partition(dp, tid, partition);
          if(partition == -1U)
    	 break; 
    
          for(v = start_vertex[partition]; v < start_vertex[partition + 1]; v++)
          {
    	// ....
           }
       }
    }
    
    dynamic_partitioning_reset(dp);
```    



--------------------
